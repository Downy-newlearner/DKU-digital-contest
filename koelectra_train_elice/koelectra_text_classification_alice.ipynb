{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# KoELECTRAë¥¼ í™œìš©í•œ AI vs Human í…ìŠ¤íŠ¸ ë¶„ë¥˜ ëª¨ë¸ (ì•¨ë¦¬ìŠ¤ë© í™˜ê²½)\n",
        "\n",
        "ì´ ë…¸íŠ¸ë¶ì€ í•œêµ­ì–´ ELECTRA ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ AIê°€ ìƒì„±í•œ í…ìŠ¤íŠ¸ì™€ ì‚¬ëŒì´ ì‘ì„±í•œ í…ìŠ¤íŠ¸ë¥¼ ë¶„ë¥˜í•˜ëŠ” ëª¨ë¸ì„ êµ¬í˜„í•©ë‹ˆë‹¤.\n",
        "\n",
        "## ì£¼ìš” íŠ¹ì§•\n",
        "- **ëª¨ë¸**: monologg/koelectra-base-v3-discriminator\n",
        "- **í‰ê°€ ì§€í‘œ**: ROC-AUC\n",
        "- **ê¸°ë²•**: Early Stopping, Power Tuning\n",
        "- **í”„ë ˆì„ì›Œí¬**: HuggingFace Transformers\n",
        "- **í™˜ê²½**: ì•¨ë¦¬ìŠ¤ë© í´ë¼ìš°ë“œ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, roc_auc_score\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import datetime\n",
        "import warnings\n",
        "\n",
        "# ê²½ê³  ë©”ì‹œì§€ í•„í„°ë§\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ\")\n",
        "print(f\"PyTorch ë²„ì „: {torch.__version__}\")\n",
        "print(f\"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU ê°œìˆ˜: {torch.cuda.device_count()}\")\n",
        "    print(f\"í˜„ì¬ GPU: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ë°ì´í„° íŒŒì¼ ê²½ë¡œ ì„¤ì • (ë¡œì»¬ ê²½ë¡œ)\n",
        "DATA_DIR = \"./\"\n",
        "TRAIN_FILE = \"train.csv\"\n",
        "TEST_FILE = \"test.csv\"\n",
        "SUBMISSION_FILE = \"sample_submission.csv\"\n",
        "\n",
        "# ì „ì²´ ê²½ë¡œ ìƒì„±\n",
        "train_file_path = os.path.join(DATA_DIR, TRAIN_FILE)\n",
        "test_file_path = os.path.join(DATA_DIR, TEST_FILE)\n",
        "submission_file_path = os.path.join(DATA_DIR, SUBMISSION_FILE)\n",
        "\n",
        "print(f\"í›ˆë ¨ ë°ì´í„° ê²½ë¡œ: {train_file_path}\")\n",
        "print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œ: {test_file_path}\")\n",
        "print(f\"ìƒ˜í”Œ ì œì¶œ íŒŒì¼ ê²½ë¡œ: {submission_file_path}\")\n",
        "\n",
        "# íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸\n",
        "required_files = [train_file_path, test_file_path, submission_file_path]\n",
        "for file_path in required_files:\n",
        "    if not os.path.isfile(file_path):\n",
        "        raise FileNotFoundError(f\"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}\")\n",
        "\n",
        "print(\"âœ… ëª¨ë“  ë°ì´í„° íŒŒì¼ í™•ì¸ ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ë°ì´í„° ë¡œë“œ\n",
        "print(\"ğŸ“Š ë°ì´í„° ë¡œë”© ì¤‘...\")\n",
        "train_data = pd.read_csv(train_file_path, encoding='utf-8')\n",
        "test_data = pd.read_csv(test_file_path, encoding='utf-8')\n",
        "submission_data = pd.read_csv(submission_file_path, encoding='utf-8')\n",
        "\n",
        "# ë°ì´í„° ì •ë³´ ì¶œë ¥\n",
        "print(\"ğŸ“Š ë°ì´í„°ì…‹ ì •ë³´:\")\n",
        "print(f\"Train ë°ì´í„°: {train_data.shape}\")\n",
        "print(f\"Test ë°ì´í„°: {test_data.shape}\")\n",
        "print(f\"Submission ë°ì´í„°: {submission_data.shape}\")\n",
        "\n",
        "print(\"\\nğŸ“ Train ë°ì´í„° ì»¬ëŸ¼:\", list(train_data.columns))\n",
        "print(\"ğŸ“ Test ë°ì´í„° ì»¬ëŸ¼:\", list(test_data.columns))\n",
        "\n",
        "print(\"\\nğŸ¯ íƒ€ê²Ÿ ë¶„í¬:\")\n",
        "print(train_data['generated'].value_counts())\n",
        "print(f\"\\ní´ë˜ìŠ¤ ë¹„ìœ¨ (Human:AI) = {train_data['generated'].value_counts()[0]}:{train_data['generated'].value_counts()[1]}\")\n",
        "\n",
        "# ë°ì´í„° ê¸°ë³¸ ì •ë³´\n",
        "print(\"\\nğŸ“‹ ë°ì´í„° ê¸°ë³¸ ì •ë³´:\")\n",
        "print(f\"í›ˆë ¨ ë°ì´í„° ê²°ì¸¡ê°’: {train_data.isnull().sum().sum()}\")\n",
        "print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²°ì¸¡ê°’: {test_data.isnull().sum().sum()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ëª¨ë¸ ì„¤ì •\n",
        "MODEL_NAME = \"monologg/koelectra-base-v3-discriminator\"\n",
        "NUM_CLASSES = 2\n",
        "MAX_SEQUENCE_LENGTH = 512\n",
        "\n",
        "print(f\"ğŸ¤– ì‚¬ìš©í•  ëª¨ë¸: {MODEL_NAME}\")\n",
        "print(f\"ğŸ“ ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´: {MAX_SEQUENCE_LENGTH}\")\n",
        "\n",
        "# í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
        "print(\"ğŸ”§ í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...\")\n",
        "text_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "print(\"âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ\")\n",
        "\n",
        "# ë¶„ë¥˜ ëª¨ë¸ ë¡œë“œ\n",
        "print(\"ğŸ”§ ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
        "classification_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=NUM_CLASSES\n",
        ")\n",
        "print(\"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")\n",
        "\n",
        "# GPU ì‚¬ìš© ì„¤ì •\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "classification_model.to(device)\n",
        "print(f\"ğŸ’» ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}\")\n",
        "\n",
        "# ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜ í™•ì¸\n",
        "total_params = sum(p.numel() for p in classification_model.parameters())\n",
        "trainable_params = sum(p.numel() for p in classification_model.parameters() if p.requires_grad)\n",
        "print(f\"ğŸ”¢ ì´ íŒŒë¼ë¯¸í„° ìˆ˜: {total_params:,}\")\n",
        "print(f\"ğŸ”¢ í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜: {trainable_params:,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomTextDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    í…ìŠ¤íŠ¸ ë¶„ë¥˜ë¥¼ ìœ„í•œ ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ í´ë˜ìŠ¤\n",
        "    \"\"\"\n",
        "    def __init__(self, text_list, label_list=None, max_seq_len=512):\n",
        "        print(f\"ğŸ“ ë°ì´í„°ì…‹ ìƒì„± ì¤‘... (ìƒ˜í”Œ ìˆ˜: {len(text_list)})\")\n",
        "        self.text_encodings = text_tokenizer(\n",
        "            text_list,\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            max_length=max_seq_len,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        self.target_labels = label_list\n",
        "        print(f\"âœ… í† í°í™” ì™„ë£Œ\")\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # ì¸ì½”ë”©ëœ í…ìŠ¤íŠ¸ ë°ì´í„° ì¶”ì¶œ\n",
        "        data_item = {\n",
        "            key: tensor[index] for key, tensor in self.text_encodings.items()\n",
        "        }\n",
        "\n",
        "        # ë¼ë²¨ì´ ìˆëŠ” ê²½ìš° ì¶”ê°€\n",
        "        if self.target_labels is not None:\n",
        "            data_item[\"labels\"] = torch.tensor(self.target_labels[index], dtype=torch.long)\n",
        "\n",
        "        return data_item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text_encodings[\"input_ids\"])\n",
        "\n",
        "print(\"âœ… ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í…ìŠ¤íŠ¸ì™€ ë¼ë²¨ ì¶”ì¶œ\n",
        "print(\"ğŸ“ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...\")\n",
        "text_samples = train_data[\"full_text\"].tolist()\n",
        "label_samples = train_data[\"generated\"].tolist()\n",
        "\n",
        "print(f\"ì´ ìƒ˜í”Œ ìˆ˜: {len(text_samples)}\")\n",
        "print(f\"í‰ê·  í…ìŠ¤íŠ¸ ê¸¸ì´: {np.mean([len(text) for text in text_samples]):.1f}ì\")\n",
        "\n",
        "# í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„í¬ í™•ì¸\n",
        "text_lengths = [len(text) for text in text_samples]\n",
        "print(f\"í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„í¬:\")\n",
        "print(f\"  - ìµœì†Œ: {min(text_lengths)}ì\")\n",
        "print(f\"  - í‰ê· : {np.mean(text_lengths):.1f}ì\")\n",
        "print(f\"  - ìµœëŒ€: {max(text_lengths)}ì\")\n",
        "print(f\"  - ì¤‘ê°„ê°’: {np.median(text_lengths):.1f}ì\")\n",
        "\n",
        "# í›ˆë ¨/ê²€ì¦ ë°ì´í„° ë¶„í•  (95:5 ë¹„ìœ¨)\n",
        "X_train, X_validation, y_train, y_validation = train_test_split(\n",
        "    text_samples,\n",
        "    label_samples,\n",
        "    test_size=0.05,\n",
        "    stratify=label_samples,  # í´ë˜ìŠ¤ ë¹„ìœ¨ ìœ ì§€\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"\\nğŸ“Š ë°ì´í„° ë¶„í•  ê²°ê³¼:\")\n",
        "print(f\"í›ˆë ¨ ë°ì´í„°: {len(X_train)}ê°œ\")\n",
        "print(f\"ê²€ì¦ ë°ì´í„°: {len(X_validation)}ê°œ\")\n",
        "print(f\"í›ˆë ¨ ë°ì´í„° í´ë˜ìŠ¤ ë¶„í¬: {np.bincount(y_train)}\")\n",
        "print(f\"ê²€ì¦ ë°ì´í„° í´ë˜ìŠ¤ ë¶„í¬: {np.bincount(y_validation)}\")\n",
        "\n",
        "# ë°ì´í„°ì…‹ ê°ì²´ ìƒì„±\n",
        "print(\"\\nğŸ”§ ë°ì´í„°ì…‹ ê°ì²´ ìƒì„± ì¤‘...\")\n",
        "print(\"ğŸ”„ í›ˆë ¨ ë°ì´í„°ì…‹ ìƒì„±...\")\n",
        "training_dataset = CustomTextDataset(X_train, y_train, MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "print(\"ğŸ”„ ê²€ì¦ ë°ì´í„°ì…‹ ìƒì„±...\")\n",
        "validation_dataset = CustomTextDataset(X_validation, y_validation, MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "print(\"ğŸ”„ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„±...\")\n",
        "test_dataset = CustomTextDataset(test_data[\"paragraph_text\"].tolist(), max_seq_len=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "print(\"âœ… ëª¨ë“  ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í›ˆë ¨ íŒŒë¼ë¯¸í„° ì„¤ì •\n",
        "EPOCHS = 10\n",
        "BATCH_SIZE = 16  # ì•¨ë¦¬ìŠ¤ë© í™˜ê²½ì— ë§ê²Œ ì¡°ì •\n",
        "LEARNING_RATE = 2e-5\n",
        "OUTPUT_DIR = \"./results\"\n",
        "LOG_DIR = \"./logs\"\n",
        "\n",
        "# ë””ë ‰í† ë¦¬ ìƒì„±\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(LOG_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"ğŸ”§ í›ˆë ¨ ì„¤ì •:\")\n",
        "print(f\"  - ì—í¬í¬: {EPOCHS}\")\n",
        "print(f\"  - ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE}\")\n",
        "print(f\"  - í•™ìŠµë¥ : {LEARNING_RATE}\")\n",
        "print(f\"  - ì¶œë ¥ ë””ë ‰í† ë¦¬: {OUTPUT_DIR}\")\n",
        "print(f\"  - ë¡œê·¸ ë””ë ‰í† ë¦¬: {LOG_DIR}\")\n",
        "\n",
        "# ì˜ˆìƒ í›ˆë ¨ ì‹œê°„ ê³„ì‚°\n",
        "steps_per_epoch = len(X_train) // BATCH_SIZE\n",
        "total_steps = steps_per_epoch * EPOCHS\n",
        "print(f\"  - ì—í¬í¬ë‹¹ ìŠ¤í… ìˆ˜: {steps_per_epoch}\")\n",
        "print(f\"  - ì´ í›ˆë ¨ ìŠ¤í… ìˆ˜: {total_steps}\")\n",
        "\n",
        "# TrainingArguments ì„¤ì •\n",
        "try:\n",
        "    model_training_args = TrainingArguments(\n",
        "        output_dir=OUTPUT_DIR,\n",
        "        num_train_epochs=EPOCHS,\n",
        "        per_device_train_batch_size=BATCH_SIZE,\n",
        "        per_device_eval_batch_size=BATCH_SIZE,\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"roc-auc\",\n",
        "        greater_is_better=True,\n",
        "        logging_dir=LOG_DIR,\n",
        "        logging_steps=100,\n",
        "        save_total_limit=3,\n",
        "        report_to=\"none\",\n",
        "        seed=42,\n",
        "        fp16=torch.cuda.is_available(),  # GPU ì‚¬ìš© ì‹œ mixed precision\n",
        "        dataloader_num_workers=2,\n",
        "        remove_unused_columns=False,\n",
        "        warmup_steps=int(total_steps * 0.1),  # 10% ì›Œë°ì—…\n",
        "        weight_decay=0.01\n",
        "    )\n",
        "    print(\"âœ… ìµœì‹  ë²„ì „ TrainingArguments ì‚¬ìš©\")\n",
        "except TypeError:\n",
        "    # êµ¬ë²„ì „ í˜¸í™˜\n",
        "    model_training_args = TrainingArguments(\n",
        "        output_dir=OUTPUT_DIR,\n",
        "        num_train_epochs=EPOCHS,\n",
        "        per_device_train_batch_size=BATCH_SIZE,\n",
        "        per_device_eval_batch_size=BATCH_SIZE,\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"roc-auc\",\n",
        "        greater_is_better=True,\n",
        "        logging_dir=LOG_DIR,\n",
        "        logging_steps=100,\n",
        "        save_total_limit=3,\n",
        "        report_to=\"none\",\n",
        "        seed=42\n",
        "    )\n",
        "    print(\"âœ… êµ¬ë²„ì „ TrainingArguments ì‚¬ìš©\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_evaluation_metrics(evaluation_predictions):\n",
        "    \"\"\"\n",
        "    ëª¨ë¸ í‰ê°€ë¥¼ ìœ„í•œ ë©”íŠ¸ë¦­ ê³„ì‚° í•¨ìˆ˜\n",
        "    \"\"\"\n",
        "    logits, true_labels = evaluation_predictions\n",
        "\n",
        "    # ì˜ˆì¸¡ í´ë˜ìŠ¤ ê³„ì‚°\n",
        "    predicted_classes = torch.argmax(torch.tensor(logits), dim=1).numpy()\n",
        "\n",
        "    # í™•ë¥  ê³„ì‚° (í´ë˜ìŠ¤ 1ì— ëŒ€í•œ í™•ë¥ )\n",
        "    class_probabilities = F.softmax(torch.tensor(logits), dim=1)[:, 1].numpy()\n",
        "\n",
        "    # F1 ì ìˆ˜ ê³„ì‚°\n",
        "    f1_score_value = f1_score(true_labels, predicted_classes)\n",
        "\n",
        "    # ì •í™•ë„ ê³„ì‚°\n",
        "    accuracy_value = (predicted_classes == true_labels).mean()\n",
        "\n",
        "    # ROC-AUC ê³„ì‚°\n",
        "    roc_auc_value = roc_auc_score(true_labels, class_probabilities)\n",
        "\n",
        "    # ì¶”ê°€ ë©”íŠ¸ë¦­ ê³„ì‚°\n",
        "    from sklearn.metrics import precision_score, recall_score\n",
        "    precision_value = precision_score(true_labels, predicted_classes)\n",
        "    recall_value = recall_score(true_labels, predicted_classes)\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy_value,\n",
        "        \"f1\": f1_score_value,\n",
        "        \"precision\": precision_value,\n",
        "        \"recall\": recall_value,\n",
        "        \"roc-auc\": roc_auc_value\n",
        "    }\n",
        "\n",
        "print(\"âœ… í‰ê°€ ë©”íŠ¸ë¦­ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Trainer ê°ì²´ ìƒì„±\n",
        "print(\"ğŸ”§ Trainer ì„¤ì • ì¤‘...\")\n",
        "model_trainer = Trainer(\n",
        "    model=classification_model,\n",
        "    args=model_training_args,\n",
        "    train_dataset=training_dataset,\n",
        "    eval_dataset=validation_dataset,\n",
        "    compute_metrics=calculate_evaluation_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        ")\n",
        "\n",
        "print(\"âœ… Trainer ì„¤ì • ì™„ë£Œ\")\n",
        "print(\"ğŸš€ ëª¨ë¸ í›ˆë ¨ ì‹œì‘...\")\n",
        "print(f\"ì˜ˆìƒ í›ˆë ¨ ì‹œê°„: ì•½ {(len(X_train) // BATCH_SIZE) * EPOCHS * 2 // 60}ë¶„\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# ëª¨ë¸ í›ˆë ¨ ì‹¤í–‰\n",
        "start_time = datetime.datetime.now()\n",
        "training_results = model_trainer.train()\n",
        "end_time = datetime.datetime.now()\n",
        "\n",
        "training_duration = (end_time - start_time).total_seconds()\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"âœ… ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!\")\n",
        "print(f\"ğŸ“ˆ ìµœì¢… í›ˆë ¨ ì†ì‹¤: {training_results.training_loss:.4f}\")\n",
        "print(f\"â±ï¸ í›ˆë ¨ ì†Œìš” ì‹œê°„: {training_duration // 60:.0f}ë¶„ {training_duration % 60:.0f}ì´ˆ\")\n",
        "print(f\"ğŸ”„ ì´ í›ˆë ¨ ìŠ¤í…: {training_results.global_step}\")\n",
        "print(f\"âš¡ í‰ê·  ì†ë„: {training_results.global_step / training_duration:.2f} steps/sec\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ê²€ì¦ ë°ì´í„°ë¡œ ìµœì¢… í‰ê°€\n",
        "print(\"ğŸ“Š ê²€ì¦ ë°ì´í„°ë¡œ ìµœì¢… í‰ê°€ ì¤‘...\")\n",
        "eval_results = model_trainer.evaluate()\n",
        "print(\"\\nğŸ¯ ìµœì¢… ê²€ì¦ ê²°ê³¼:\")\n",
        "print(\"=\" * 40)\n",
        "for key, value in eval_results.items():\n",
        "    if key.startswith('eval_'):\n",
        "        metric_name = key.replace('eval_', '').upper()\n",
        "        print(f\"  {metric_name}: {value:.4f}\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ ìˆ˜í–‰\n",
        "print(\"\\nğŸ”® í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ì¤‘...\")\n",
        "test_predictions = model_trainer.predict(test_dataset)\n",
        "\n",
        "# í´ë˜ìŠ¤ 1ì— ëŒ€í•œ í™•ë¥  ê³„ì‚°\n",
        "raw_probabilities = F.softmax(torch.tensor(test_predictions.predictions), dim=1)[:, 1].numpy()\n",
        "\n",
        "print(f\"ğŸ“Š ì˜ˆì¸¡ ì™„ë£Œ - {len(raw_probabilities)}ê°œ ìƒ˜í”Œ\")\n",
        "print(f\"í™•ë¥  í†µê³„:\")\n",
        "print(f\"  - ìµœì†Œê°’: {raw_probabilities.min():.4f}\")\n",
        "print(f\"  - ìµœëŒ€ê°’: {raw_probabilities.max():.4f}\")\n",
        "print(f\"  - í‰ê· ê°’: {raw_probabilities.mean():.4f}\")\n",
        "print(f\"  - ì¤‘ê°„ê°’: {np.median(raw_probabilities):.4f}\")\n",
        "print(f\"  - í‘œì¤€í¸ì°¨: {raw_probabilities.std():.4f}\")\n",
        "\n",
        "# í™•ë¥  ë¶„í¬ í™•ì¸\n",
        "print(f\"\\ní™•ë¥  ë¶„í¬:\")\n",
        "print(f\"  - 0.0-0.1: {((raw_probabilities >= 0.0) & (raw_probabilities < 0.1)).mean():.1%}\")\n",
        "print(f\"  - 0.1-0.3: {((raw_probabilities >= 0.1) & (raw_probabilities < 0.3)).mean():.1%}\")\n",
        "print(f\"  - 0.3-0.5: {((raw_probabilities >= 0.3) & (raw_probabilities < 0.5)).mean():.1%}\")\n",
        "print(f\"  - 0.5-0.7: {((raw_probabilities >= 0.5) & (raw_probabilities < 0.7)).mean():.1%}\")\n",
        "print(f\"  - 0.7-0.9: {((raw_probabilities >= 0.7) & (raw_probabilities < 0.9)).mean():.1%}\")\n",
        "print(f\"  - 0.9-1.0: {((raw_probabilities >= 0.9) & (raw_probabilities <= 1.0)).mean():.1%}\")\n",
        "\n",
        "# íŒŒì›Œ íŠœë‹ ì ìš©\n",
        "POWER_TUNING_ALPHA = 1.1  # íŒŒì›Œ íŠœë‹ ê³„ìˆ˜\n",
        "tuned_probabilities = np.clip(raw_probabilities ** POWER_TUNING_ALPHA, 0, 1)\n",
        "\n",
        "print(f\"\\nâš¡ íŒŒì›Œ íŠœë‹ ì ìš© (Î±={POWER_TUNING_ALPHA})\")\n",
        "print(f\"íŠœë‹ í›„ í™•ë¥  í†µê³„:\")\n",
        "print(f\"  - ìµœì†Œê°’: {tuned_probabilities.min():.4f}\")\n",
        "print(f\"  - ìµœëŒ€ê°’: {tuned_probabilities.max():.4f}\")\n",
        "print(f\"  - í‰ê· ê°’: {tuned_probabilities.mean():.4f}\")\n",
        "print(f\"  - ì¤‘ê°„ê°’: {np.median(tuned_probabilities):.4f}\")\n",
        "print(f\"  - í‘œì¤€í¸ì°¨: {tuned_probabilities.std():.4f}\")\n",
        "\n",
        "# ì œì¶œ ë°ì´í„°ì— ì˜ˆì¸¡ ê²°ê³¼ í• ë‹¹\n",
        "submission_data[\"generated\"] = tuned_probabilities\n",
        "print(\"âœ… ì˜ˆì¸¡ ê²°ê³¼ í• ë‹¹ ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ì œì¶œ íŒŒì¼ ì €ì¥\n",
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "auc_score = eval_results.get('eval_roc-auc', 0)\n",
        "submission_filename = f\"koelectra_submission_{current_time}_auc_{auc_score:.4f}.csv\"\n",
        "final_submission_path = os.path.join(\"./\", submission_filename)\n",
        "\n",
        "# ì œì¶œ íŒŒì¼ ì €ì¥\n",
        "submission_data.to_csv(final_submission_path, index=False)\n",
        "\n",
        "print(\"ğŸ‰ ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ!\")\n",
        "print(f\"ğŸ“ íŒŒì¼ ê²½ë¡œ: {final_submission_path}\")\n",
        "print(f\"ğŸ“Š ì œì¶œ ë°ì´í„° í¬ê¸°: {submission_data.shape}\")\n",
        "\n",
        "# ì˜ˆì¸¡ ê²°ê³¼ ìš”ì•½ ì¶œë ¥\n",
        "print(\"\\nğŸ“ˆ ì˜ˆì¸¡ ê²°ê³¼ ìš”ì•½:\")\n",
        "print(\"=\" * 30)\n",
        "print(f\"í‰ê·  í™•ë¥ : {tuned_probabilities.mean():.4f}\")\n",
        "print(f\"í‘œì¤€í¸ì°¨: {tuned_probabilities.std():.4f}\")\n",
        "print(f\"AI ìƒì„± ì˜ˆì¸¡ ë¹„ìœ¨:\")\n",
        "print(f\"  - í™•ë¥  > 0.5: {(tuned_probabilities > 0.5).mean():.1%}\")\n",
        "print(f\"  - í™•ë¥  > 0.3: {(tuned_probabilities > 0.3).mean():.1%}\")\n",
        "print(f\"  - í™•ë¥  > 0.7: {(tuned_probabilities > 0.7).mean():.1%}\")\n",
        "print(\"=\" * 30)\n",
        "\n",
        "# ì œì¶œ íŒŒì¼ ë¯¸ë¦¬ë³´ê¸°\n",
        "print(\"\\nğŸ‘€ ì œì¶œ íŒŒì¼ ë¯¸ë¦¬ë³´ê¸°:\")\n",
        "print(submission_data.head(10))\n",
        "\n",
        "# ëª¨ë¸ ì €ì¥ (ì„ íƒì‚¬í•­)\n",
        "model_save_path = f\"./koelectra_model_{current_time}\"\n",
        "print(f\"\\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘: {model_save_path}\")\n",
        "model_trainer.save_model(model_save_path)\n",
        "text_tokenizer.save_pretrained(model_save_path)\n",
        "print(f\"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ì‹¤í—˜ ê²°ê³¼ ìš”ì•½\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸ¯ ì‹¤í—˜ ê²°ê³¼ ìš”ì•½\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ê¸°ë³¸ ì •ë³´\n",
        "print(\"ğŸ“‹ ê¸°ë³¸ ì •ë³´:\")\n",
        "print(f\"  - ëª¨ë¸: {MODEL_NAME}\")\n",
        "print(f\"  - ì‹¤í–‰ í™˜ê²½: ì•¨ë¦¬ìŠ¤ë© í´ë¼ìš°ë“œ\")\n",
        "print(f\"  - ì‹¤í–‰ ì‹œê°„: {current_time}\")\n",
        "print(f\"  - ë””ë°”ì´ìŠ¤: {device}\")\n",
        "\n",
        "# ë°ì´í„° ì •ë³´\n",
        "print(\"\\nğŸ“Š ë°ì´í„° ì •ë³´:\")\n",
        "print(f\"  - í›ˆë ¨ ë°ì´í„° í¬ê¸°: {len(X_train):,}\")\n",
        "print(f\"  - ê²€ì¦ ë°ì´í„° í¬ê¸°: {len(X_validation):,}\")\n",
        "print(f\"  - í…ŒìŠ¤íŠ¸ ë°ì´í„° í¬ê¸°: {len(test_data):,}\")\n",
        "print(f\"  - í´ë˜ìŠ¤ ë¶„í¬ (Human:AI): {train_data['generated'].value_counts()[0]}:{train_data['generated'].value_counts()[1]}\")\n",
        "\n",
        "# í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
        "print(\"\\nâš™ï¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°:\")\n",
        "print(f\"  - ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE}\")\n",
        "print(f\"  - í•™ìŠµë¥ : {LEARNING_RATE}\")\n",
        "print(f\"  - ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´: {MAX_SEQUENCE_LENGTH}\")\n",
        "print(f\"  - ì—í¬í¬ ìˆ˜: {EPOCHS}\")\n",
        "print(f\"  - íŒŒì›Œ íŠœë‹ ê³„ìˆ˜: {POWER_TUNING_ALPHA}\")\n",
        "\n",
        "# í›ˆë ¨ ê²°ê³¼\n",
        "print(\"\\nğŸ† í›ˆë ¨ ê²°ê³¼:\")\n",
        "print(f\"  - ìµœì¢… í›ˆë ¨ ì†ì‹¤: {training_results.training_loss:.4f}\")\n",
        "print(f\"  - í›ˆë ¨ ì‹œê°„: {training_duration // 60:.0f}ë¶„ {training_duration % 60:.0f}ì´ˆ\")\n",
        "print(f\"  - ì´ í›ˆë ¨ ìŠ¤í…: {training_results.global_step}\")\n",
        "\n",
        "# ê²€ì¦ ì„±ëŠ¥\n",
        "print(\"\\nğŸ“ˆ ê²€ì¦ ì„±ëŠ¥:\")\n",
        "for key, value in eval_results.items():\n",
        "    if key.startswith('eval_'):\n",
        "        metric_name = key.replace('eval_', '').upper()\n",
        "        print(f\"  - {metric_name}: {value:.4f}\")\n",
        "\n",
        "# ì œì¶œ ì •ë³´\n",
        "print(\"\\nğŸ“¤ ì œì¶œ ì •ë³´:\")\n",
        "print(f\"  - ì œì¶œ íŒŒì¼: {submission_filename}\")\n",
        "print(f\"  - ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {model_save_path}\")\n",
        "print(f\"  - ì˜ˆì¸¡ í‰ê·  í™•ë¥ : {tuned_probabilities.mean():.4f}\")\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"âœ… ì‹¤í—˜ ì™„ë£Œ!\")\n",
        "print(\"=\"*60)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
