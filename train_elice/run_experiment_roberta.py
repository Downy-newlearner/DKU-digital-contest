#!/usr/bin/env python3
"""
KoELECTRA AI vs Human í…ìŠ¤íŠ¸ ë¶„ë¥˜ ëª¨ë¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
klue/roberta-large ëª¨ë¸ ì‚¬ìš© ë° ì—„ê²©í•œ ì •ì œ ë°ì´í„° ì ìš©
"""

import os
import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, roc_auc_score, precision_score, recall_score, accuracy_score
import torch.nn.functional as F
import numpy as np
import datetime
import warnings
import sys

# ê²½ê³  ë©”ì‹œì§€ í•„í„°ë§
warnings.filterwarnings('ignore')

def print_section(title):
    """ì„¹ì…˜ ì œëª©ì„ ì¶œë ¥í•©ë‹ˆë‹¤."""
    print(f"\n{'='*60}")
    print(f"ğŸ¯ {title}")
    print(f"{'='*60}")

def main():
    print_section("KoELECTRA RoBERTa-Large í…ìŠ¤íŠ¸ ë¶„ë¥˜ ëª¨ë¸ ì‹¤í–‰ ì‹œì‘")
    
    # 1. í™˜ê²½ í™•ì¸
    print("âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ")
    print(f"PyTorch ë²„ì „: {torch.__version__}")
    print(f"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"GPU ê°œìˆ˜: {torch.cuda.device_count()}")
        print(f"í˜„ì¬ GPU: {torch.cuda.get_device_name(0)}")
    
    # 2. ë°ì´í„° íŒŒì¼ í™•ì¸
    print_section("ë°ì´í„° íŒŒì¼ í™•ì¸")
    
    DATA_DIR = "./"
    TRAIN_FILE = "train_strictly_cleaned.csv"  # ì—„ê²©í•œ ì •ì œ ë°ì´í„° ì‚¬ìš©
    TEST_FILE = "test.csv"
    SUBMISSION_FILE = "sample_submission.csv"
    
    train_file_path = os.path.join(DATA_DIR, TRAIN_FILE)
    test_file_path = os.path.join(DATA_DIR, TEST_FILE)
    submission_file_path = os.path.join(DATA_DIR, SUBMISSION_FILE)
    
    required_files = [train_file_path, test_file_path, submission_file_path]
    for file_path in required_files:
        if not os.path.isfile(file_path):
            print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
            sys.exit(1)
    
    print("âœ… ëª¨ë“  ë°ì´í„° íŒŒì¼ í™•ì¸ ì™„ë£Œ")
    print(f"ğŸ“Š ì‚¬ìš©í•  í›ˆë ¨ ë°ì´í„°: {TRAIN_FILE}")
    
    # 3. ë°ì´í„° ë¡œë“œ
    print_section("ë°ì´í„° ë¡œë“œ")
    
    train_data = pd.read_csv(train_file_path, encoding='utf-8')
    test_data = pd.read_csv(test_file_path, encoding='utf-8')
    submission_data = pd.read_csv(submission_file_path, encoding='utf-8')
    
    print(f"Train ë°ì´í„°: {train_data.shape}")
    print(f"Test ë°ì´í„°: {test_data.shape}")
    print(f"Submission ë°ì´í„°: {submission_data.shape}")
    print(f"í´ë˜ìŠ¤ ë¶„í¬: {train_data['generated'].value_counts().to_dict()}")
    
    # 4. ëª¨ë¸ ì„¤ì •
    print_section("ëª¨ë¸ ì„¤ì •")
    
    MODEL_NAME = "klue/roberta-large"  # RoBERTa-Large ì‚¬ìš©
    NUM_CLASSES = 2
    MAX_SEQUENCE_LENGTH = 512
    
    print(f"ğŸ¤– ëª¨ë¸: {MODEL_NAME}")
    print(f"ğŸ“ ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´: {MAX_SEQUENCE_LENGTH}")
    
    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    text_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    print("âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ")
    
    # ëª¨ë¸ ë¡œë“œ
    classification_model = AutoModelForSequenceClassification.from_pretrained(
        MODEL_NAME, num_labels=NUM_CLASSES
    )
    print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    # GPU ì„¤ì •
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    classification_model.to(device)
    print(f"ğŸ’» ë””ë°”ì´ìŠ¤: {device}")
    
    # 5. ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜
    class CustomTextDataset(torch.utils.data.Dataset):
        def __init__(self, text_list, label_list=None, max_seq_len=512):
            self.text_encodings = text_tokenizer(
                text_list,
                truncation=True,
                padding=True,
                max_length=max_seq_len,
                return_tensors="pt"
            )
            self.target_labels = label_list

        def __getitem__(self, index):
            data_item = {
                key: tensor[index] for key, tensor in self.text_encodings.items()
            }
            if self.target_labels is not None:
                data_item["labels"] = torch.tensor(self.target_labels[index], dtype=torch.long)
            return data_item

        def __len__(self):
            return len(self.text_encodings["input_ids"])
    
    # 6. ë°ì´í„° ì „ì²˜ë¦¬
    print_section("ë°ì´í„° ì „ì²˜ë¦¬")
    
    text_samples = train_data["full_text"].tolist()
    label_samples = train_data["generated"].tolist()
    
    # í›ˆë ¨/ê²€ì¦ ë¶„í• 
    X_train, X_validation, y_train, y_validation = train_test_split(
        text_samples, label_samples, test_size=0.1, stratify=label_samples, random_state=42
    )
    
    print(f"í›ˆë ¨ ë°ì´í„°: {len(X_train):,}ê°œ")
    print(f"ê²€ì¦ ë°ì´í„°: {len(X_validation):,}ê°œ")
    
    # ë°ì´í„°ì…‹ ìƒì„±
    training_dataset = CustomTextDataset(X_train, y_train, MAX_SEQUENCE_LENGTH)
    validation_dataset = CustomTextDataset(X_validation, y_validation, MAX_SEQUENCE_LENGTH)
    test_dataset = CustomTextDataset(test_data["paragraph_text"].tolist(), max_seq_len=MAX_SEQUENCE_LENGTH)
    
    print("âœ… ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ")
    
    # 7. í›ˆë ¨ ì„¤ì •
    print_section("í›ˆë ¨ ì„¤ì •")
    
    EPOCHS = 5  # RoBERTa-LargeëŠ” ë” ì ì€ ì—í¬í¬ë¡œë„ ì¢‹ì€ ì„±ëŠ¥
    BATCH_SIZE = 8  # ë©”ëª¨ë¦¬ ê³ ë ¤í•˜ì—¬ ë°°ì¹˜ í¬ê¸° ê°ì†Œ
    LEARNING_RATE = 1e-5  # RoBERTa-LargeëŠ” ë” ì‘ì€ í•™ìŠµë¥  ì‚¬ìš©
    OUTPUT_DIR = "./results_roberta_large"
    LOG_DIR = "./logs_roberta_large"
    
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    os.makedirs(LOG_DIR, exist_ok=True)
    
    print(f"ì—í¬í¬: {EPOCHS}")
    print(f"ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE}")
    print(f"í•™ìŠµë¥ : {LEARNING_RATE}")
    print(f"ì¶œë ¥ ë””ë ‰í† ë¦¬: {OUTPUT_DIR}")
    
    # 8. í‰ê°€ ë©”íŠ¸ë¦­ í•¨ìˆ˜
    def calculate_evaluation_metrics(evaluation_predictions):
        predictions, labels = evaluation_predictions
        predictions = np.argmax(predictions, axis=1)
        
        return {
            "accuracy": accuracy_score(labels, predictions),
            "f1": f1_score(labels, predictions),
            "precision": precision_score(labels, predictions),
            "recall": recall_score(labels, predictions),
            "roc_auc": roc_auc_score(labels, 
                                   F.softmax(torch.tensor(evaluation_predictions.predictions), dim=1)[:, 1].numpy())
        }
    
    # 9. TrainingArguments ì„¤ì •
    model_training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        num_train_epochs=EPOCHS,
        per_device_train_batch_size=BATCH_SIZE,
        per_device_eval_batch_size=BATCH_SIZE,
        learning_rate=LEARNING_RATE,
        weight_decay=0.01,
        evaluation_strategy="epoch",
        save_strategy="epoch",
        load_best_model_at_end=True,
        metric_for_best_model="roc_auc",
        greater_is_better=True,
        logging_dir=LOG_DIR,
        logging_steps=100,
        save_total_limit=3,
        report_to="none",
        seed=42,
        fp16=torch.cuda.is_available(),
        dataloader_num_workers=4,
        remove_unused_columns=False,
        warmup_steps=500,
        gradient_accumulation_steps=2
    )
    
    # 10. í›ˆë ¨ ì‹¤í–‰
    print_section("ëª¨ë¸ í›ˆë ¨")
    
    model_trainer = Trainer(
        model=classification_model,
        args=model_training_args,
        train_dataset=training_dataset,
        eval_dataset=validation_dataset,
        compute_metrics=calculate_evaluation_metrics,
        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]
    )
    
    start_time = datetime.datetime.now()
    print(f"ğŸš€ í›ˆë ¨ ì‹œì‘: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    training_results = model_trainer.train()
    
    end_time = datetime.datetime.now()
    training_duration = (end_time - start_time).total_seconds()
    
    print(f"âœ… í›ˆë ¨ ì™„ë£Œ!")
    print(f"í›ˆë ¨ ì‹œê°„: {training_duration // 60:.0f}ë¶„ {training_duration % 60:.0f}ì´ˆ")
    
    # 11. í‰ê°€ ë° ì˜ˆì¸¡
    print_section("í‰ê°€ ë° ì˜ˆì¸¡")
    
    eval_results = model_trainer.evaluate()
    print("ğŸ” ê²€ì¦ ê²°ê³¼:")
    for key, value in eval_results.items():
        if key.startswith('eval_'):
            print(f"  {key}: {value:.4f}")
    
    # í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡
    test_predictions = model_trainer.predict(test_dataset)
    raw_probabilities = F.softmax(torch.tensor(test_predictions.predictions), dim=1)[:, 1].numpy()
    
    # 12. ì œì¶œ íŒŒì¼ ìƒì„±
    print_section("ì œì¶œ íŒŒì¼ ìƒì„±")
    
    current_time = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    auc_score = eval_results.get('eval_roc_auc', 0)
    submission_filename = f"roberta_large_strictly_cleaned_{current_time}_auc_{auc_score:.4f}.csv"
    
    submission_data["generated"] = raw_probabilities
    submission_data.to_csv(submission_filename, index=False)
    
    print(f"âœ… ì œì¶œ íŒŒì¼ ì €ì¥: {submission_filename}")
    
    # 13. ëª¨ë¸ ì €ì¥
    model_save_path = f"./roberta_large_model_{current_time}"
    model_trainer.save_model(model_save_path)
    text_tokenizer.save_pretrained(model_save_path)
    
    print(f"âœ… ëª¨ë¸ ì €ì¥: {model_save_path}")
    
    # 14. ê²°ê³¼ ìš”ì•½
    print_section("ì‹¤í—˜ ê²°ê³¼ ìš”ì•½")
    
    print(f"ğŸ¤– ëª¨ë¸: {MODEL_NAME}")
    print(f"ğŸ“Š í›ˆë ¨ ë°ì´í„°: {TRAIN_FILE}")
    print(f"ğŸ“ˆ í›ˆë ¨ ìƒ˜í”Œ: {len(X_train):,}ê°œ")
    print(f"ğŸ” ê²€ì¦ ìƒ˜í”Œ: {len(X_validation):,}ê°œ")
    print(f"ğŸ¯ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ: {len(test_data):,}ê°œ")
    print(f"â±ï¸ í›ˆë ¨ ì‹œê°„: {training_duration // 60:.0f}ë¶„ {training_duration % 60:.0f}ì´ˆ")
    print(f"ğŸ‰ ìµœì¢… ROC-AUC: {auc_score:.4f}")
    print(f"ğŸ“ ì œì¶œ íŒŒì¼: {submission_filename}")
    print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {model_save_path}")
    
    # 15. ë°ì´í„° í’ˆì§ˆ ê°œì„  íš¨ê³¼ ì¶œë ¥
    print_section("ë°ì´í„° í’ˆì§ˆ ê°œì„  íš¨ê³¼")
    
    print(f"âœ… ì—„ê²©í•œ ì •ì œ ë°ì´í„° ì‚¬ìš© íš¨ê³¼:")
    print(f"   - ë¬¸ë‹¨ ìˆ˜ 25ê°œ ë¯¸ë§Œìœ¼ë¡œ ì œí•œ")
    print(f"   - ë…¸ì´ì¦ˆ ë°ì´í„° ì™„ì „ ì œê±°")
    print(f"   - ì¼ê´€ëœ í’ˆì§ˆì˜ í…ìŠ¤íŠ¸ ë°ì´í„°")
    print(f"   - ëª¨ë¸ í•™ìŠµ ì•ˆì •ì„± í–¥ìƒ")
    
    print_section("ì‹¤í—˜ ì™„ë£Œ")
    
    return {
        'model_name': MODEL_NAME,
        'train_file': TRAIN_FILE,
        'auc_score': auc_score,
        'submission_file': submission_filename,
        'model_path': model_save_path,
        'training_time': training_duration
    }

if __name__ == "__main__":
    results = main() 